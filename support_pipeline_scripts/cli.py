import click
import re
import historydag as hdag
from historydag.parsimony import load_fasta, build_tree, disambiguate, sankoff_upward
from itertools import islice
import ete3 as ete
import random
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
import os
import dendropy
import json

import random
import subprocess
# from newick_parser.tree_transformer import iter_nexus_trees
from historydag import parsimony_utils

from math import exp

import seaborn as sns
sns.set_theme()
# plt.rcParams['text.usetex'] = True

@click.group(context_settings={"help_option_names": ["-h", "--help"]})
def cli():
    """
    Command line scripts to facilitate node support validation
    """
    pass


###################################################################################################
### Simulation Info ###############################################################################
###################################################################################################

@click.command("parse_clade_stats")
@click.option('--in_file', '-i', help='path to clade_stats.tsv generated by matUtils.')
def parse_clade_stats(in_file):
    """
    Given tsv file of clade stats, find good clades. Use this command to generate stats file:
        
        matUtils summary -i public-2022-10-01.all.masked.pb.gz -c clade_stats.tsv

    """
    
    with open(in_file, "r") as f:
        for line in f.readlines()[1:]:
            row = line.strip().split("\t")
            name = row[0]
            leaf_count = int(row[1])

            if leaf_count <= 100 and leaf_count > 50:
                print(name, "\t", leaf_count)



@click.command("get_pars_score")
@click.option('--sim_dir', '-s', help='the folder containing TOI and fasta file.')
def get_pars_score(sim_dir):
    """
    Computes the parsimony score of the simulated tree, the maximum possible parsimony given the
    topology, and the maximum parsimony on the leaves. Stores results as a json at sim_dir/tree_stats.json
    """

    var_sites_prefix = sim_dir + "/collapsed_simulated_tree.nwk.variant_sites"
    with open(var_sites_prefix + ".txt", "r") as f:
        idxs = f.readline().split()

    with open(sim_dir + "/ctree_with_refseq.fasta", "r") as f:
        name = f.readline().strip()[1:] # NOTE: Should be `ancestral`
        seq = f.readline()              # Entire sequence
        variants = ""                   # The character values of the nucs that vary
        for i in idxs:
            variants += seq[int(i)-1]

    with open(var_sites_prefix + ".fasta", "r") as f:
        var_sites = f.readlines()

    with open(var_sites_prefix + "_with_refseq.fasta", "w") as f:
        f.write(f">{name}\n")
        f.write(f"{variants}\n")
        for line in var_sites:
            f.write(f"{line}")

    # This ouputs to simdir/dnapars_output.txt
    subprocess.run([
        "dnapars_parsimony_score.sh",
        var_sites_prefix + "_with_refseq.fasta",    # infasta
        name,                                       # root_name
        sim_dir                                     # out_dir
    ])

    with open(sim_dir + "/dnapars_output.txt", "r") as f:
        line = f.readline()
        temp = line.strip().split(" ")
        best_possible = float(temp[-1])



    from historydag.parsimony import parsimony_score, sankoff_upward
    tree_path = sim_dir + "/collapsed_simulated_tree.nwk"
    tree = ete.Tree(tree_path) # Doesn't have internal names

    fasta_path = sim_dir + "/ctree_with_refseq.fasta"   # ancestral seq in second line of this file
    with open(fasta_path, "r") as f:
        assert ">ancestral\n" == f.readline()
        ancestral_seq = f.readline().strip()
    
    # build sequences from mutations
    num_nodes = 0
    num_leaves = 0
    for node in tree.traverse("preorder"):
        num_nodes += 1
        if node.is_leaf():
            num_leaves += 1
        if node.is_root():
            seq = ancestral_seq
        else:
            seq = node.up.sequence
            if len(node.mutations) >= 1:
                for mut in node.mutations.split("|"):
                    mut = mut.strip()
                    curr = mut[0]
                    i = int(mut[1:-1])-1    # convert indices to 0-based
                    new = mut[-1]
                    assert seq[i] == curr
                    seq = seq[:i] + new + seq[i + 1:]
        
        node.add_feature("sequence", seq)

    # just counts mutations between simulated internal node sequences
    tree_score = parsimony_score(tree)

    # computes the best possible parsimony score of any labeling on tree's
    # topology, with root sequence constrained.
    # first replace all internal, non-root sequences with N's, then do
    # sankoff_upward with use_internal_node_sequences True.
    sankoff_tree = tree.copy()
    for node in sankoff_tree.traverse():
        if node.is_root() or node.is_leaf():
            continue
        else:
            node.sequence = "N" * len(tree.sequence)
    max_score = sankoff_upward(sankoff_tree, len(sankoff_tree.sequence), use_internal_node_sequences=True)

    stats_dict = {
        "num_leaves": num_leaves,
        "num_nodes": num_nodes,
        "pars_score": tree_score,
        "max_score_top": max_score,
        "max_score_data": best_possible
    }

    outfile = sim_dir + "/tree_stats.json"
    with open(outfile, "w") as f:
        f.write(json.dumps(stats_dict))




###################################################################################################
#### Inference ####################################################################################
###################################################################################################

@click.command("save_supports")
@click.option('--method', '-m', default="hdag", help='method of inference.')
@click.option('--tree_path', '-t', help='the newick file for the true tree.')
@click.option('--input_path', '-i', help='the file containing inference results.')
@click.option('--output_path', '-o', help='the file to save output to.')
def save_supports(method, tree_path, input_path, output_path):
    """
    A method for computing, formatting, and saving node supports for various methods of inference 
    Input:  method of inference (e.g., hdag, beast, etc.),
            input file path (e.g., to history dag, or beast output),
            output filepath 
    Output: For each file in the input directory, a text file containing list of support values in the
            format (clade, estimated_support, in_tree)
    """

    # Map of taxon id (e.g., s1, s4, etc) to full sequence
    fasta_path = tree_path + ".fasta"
    taxId2seq = hdag.parsimony.load_fasta(fasta_path)

    # Compute the set of nodes that are in the true tree
    node_set = get_true_nodes(tree_path)

    # Computes list of nodes 
    if method == "hdag":
        support_list = hdag_output(node_set, input_path, taxId2seq)
    
    elif method[0:5] == "hdag-":
        if method[5:] == "inf":
            p = "inf"
        else:
            p = float(method[5:])
        support_list = hdag_output_general(node_set, input_path, taxId2seq, pars_weight=p)
    elif method == "beast":
        support_list = beast_output(node_set, input_path)
    elif method == "mrbayes":
        support_list = mrbayes_output(node_set, input_path)
    else:
        raise Exception(f"Invalid method: {method}")

    with open(output_path, "wb") as f:
        pickle.dump(support_list, f)
    

def get_true_nodes(tree_path):
    """ Given a newick file, returns the set of all clades (as frozen sets of taxon ids)
    """
    curr_internal_name = 0
    tree = ete.Tree(tree_path)
    etenode2cu = {}
    for node in tree.traverse("postorder"):
        if node.is_leaf():
            cu = [node.name]
        else:
            if len(node.name) == 0:             # No name
                node.name = curr_internal_name
                curr_internal_name += 1
            cu = []
            for child in node.children:
                cu.extend(list(etenode2cu[child.name]))
        
        etenode2cu[node.name] = frozenset(cu)

    return set([v for k, v in etenode2cu.items()])


def hdag_output_general(node_set, pb_file, taxId2seq, pars_weight="inf"):
    """
    Uses a generalized node support that considers non-MP trees and weights them as a functions
    of their parsiomny score.

    Returns a list of tuples (clade_id, estimated_support, in_tree). The list is sorted by
    estimated support, and groups that have the same support are randomly shuffled.

    Args:
        node_set: Set of nodes (sets of taxa ids) that are in the true tree.
        pb_file: Protobuf file that contains the optimized DAG.
        taxId2Seq: Dicitonary of taxon IDs to the sequences they represent
        pars_weight: Constant to multiply by in the pscore_fn. If `inf`, then use uniform
            distribution over MP trees. Otherwise, support for edge (n1, n2) is computed as
            exp(-<pars_weight> * <pars(n1, n2)>)
    """

    dag = hdag.mutation_annotated_dag.load_MAD_protobuf_file(pb_file)

    # TODO: Try creating plots without these
    # dag.make_complete()
    dag.convert_to_collapsed()

    if pars_weight == "inf":
        # This recovers uniform distribution over MP trees
        dag.trim_optimal_weight()
        pscore_fn = lambda n1, n2: 0
    else:
        pscore_fn = lambda n1, n2: pars_weight #-pars_weight * parsimony_utils.hamming_cg_edge_weight(n1, n2)

    print("\n\tDAG contains", dag.count_trees(), "trees\n")
        
    dag.probability_annotate(lambda n1, n2: pscore_fn(n1, n2), log_probabilities=True)
    support = dag.node_probabilities(log_probabilities=True, collapse_key=lambda n: n.clade_union())


    seq2taxId = {v: k for k, v in taxId2seq.items()}

    # Construct results dict that maps nodes (frozen sets of taxon ids) to tuples of estimated
    # support and whether that node is in the true tree or not
    node2stats = {}

    # Get the support for all dag nodes
    counter = 0
    for node in support:
        if len(node) == 1:  # UA node
            continue

        # Convert cg label to taxon id
        id_node = frozenset([seq2taxId[label.compact_genome.to_sequence()] for label in node])
        est_sup = exp(support[node])    # NOTE: Using log_probabilities
        node2stats[id_node] = (est_sup, id_node in node_set)
    

    # Get the support for all nodes in true tree
    for id_node in node_set:
        if id_node not in node2stats.keys() and len(id_node) > 1:
            node2stats[id_node] = (0, True)

    print("Considering", len(node2stats), "nodes")
    stats_list =[(id_node, stats[0], stats[1]) for id_node, stats in node2stats.items()]
    random.shuffle(stats_list)
    stats_list.sort(key=lambda el: el[1])

    return stats_list


def hdag_output(node_set, pb_file, taxId2seq):
    """
    Returns a list of tuples (clade_id, estimated_support, in_tree). The list is primarily
    sorted by estimated support, and portions that have the same support are randomly shuffled
    """
    dag = hdag.mutation_annotated_dag.load_MAD_protobuf_file(pb_file)
    dag.make_complete()
    dag.trim_optimal_weight() # Trim to the MP trees
    counts = dag.count_nodes(collapse=True) # TODO: double check that this is counting the right thing
    total_trees = dag.count_trees()
    

    print("size of counts is:", len(counts))
    print(f"There are {len(node_set)} nodes in TOI")
    dag.summary()

    seq2taxId = {v: k for k, v in taxId2seq.items()}

    # Construct results dict that maps nodes (frozen sets of taxon ids) to tuples of estimated
    # support and whether that node is in the true tree or not
    node2stats = {}

    # Get the support for all dag nodes
    counter = 0
    for node in counts:
        if len(node) == 0:  # UA node
            continue

        # Convert cg label to taxon id
        id_node = frozenset([seq2taxId[label.compact_genome.to_sequence()] for label in node])
        est_sup = counts[node] / total_trees
        node2stats[id_node] = (est_sup, id_node in node_set)
    

    # Get the support for all nodes in true tree
    for id_node in node_set:
        if id_node not in node2stats.keys():
            node2stats[id_node] = (0, True)

    print("Considering", len(node2stats), "nodes")
    stats_list =[(id_node, stats[0], stats[1]) for id_node, stats in node2stats.items()]
    random.shuffle(stats_list)
    stats_list.sort(key=lambda el: el[1])

    return stats_list


def get_trprobs_trees(trprobs_file):
    with open(trprobs_file, 'r') as fh:
        for line in fh:
            if 'translate' in line:
                break
        translate_dict = {}
        for line in fh:
            idx, idx_name = line.strip().split(' ')
            translate_dict[idx] = idx_name[:-1]
            if idx_name[-1] == ';':
                break

    with open(trprobs_file, 'r') as fh:
        for line in fh:
            if 'tree' in line.strip()[0:5]:
                treeprob = float(re.search(r"(p = )([\d\.]+)", line).groups()[-1])
                cumulprob = float(re.search(r"(P = )([\d\.]+)", line).groups()[-1])
                nwk = line.strip().split(' ')[-1]
                tree = build_tree(nwk, translate_dict)
                for node in tree.iter_leaves():
                    node.name = translate_dict[node.name]
                # put original ambiguous sequences back on leaves
                print('.')
                tree.prob = treeprob
                tree.cumulprob = cumulprob
                yield tree

def reroot(new_root):
    """
    Edits the tree that the given node, new_root, is a part of so that it becomes the root.
    Returns pointer to the new root. Also, removes any unifurcations caused by edits.
    """
    node_path = [new_root]
    curr = new_root
    while not curr.is_root():
        node_path.append(curr.up)
        curr = curr.up

    root = node_path[-1]
    delete_root = len(root.children) <= 2
    
    while len(node_path) >= 2:
        curr_node = node_path[-1]
        curr_child = node_path[-2]
        curr_child.detach()
        curr_child.add_child(curr_node)
        node_path = node_path[:-1]
    if delete_root:
        root.delete()

    # NOTE: Need to delete new root's child because it will be a unifurcation
    list(curr_child.children)[0].delete()

    return curr_child
    
def make_stats_list(node2support, node_set):
    # Construct results dict that maps nodes (frozen sets of taxon ids) to tuples of estimated
    # support and whether that node is in the true tree or not
    node2stats = {}

    # Get the support for all dag nodes
    counter = 0
    for id_node, est_sup in node2support.items():
        if len(id_node) == 0:  # UA node
            continue

        node2stats[id_node] = (est_sup, id_node in node_set)
    
    # Get the support for all nodes in true tree
    for id_node in node_set:
        if id_node not in node2stats.keys():
            node2stats[id_node] = (0, True)

    print("Considering", len(node2stats), "nodes")
    stats_list =[(id_node, stats[0], stats[1]) for id_node, stats in node2stats.items()]
    random.shuffle(stats_list)
    stats_list.sort(key=lambda el: el[1])
    return stats_list

def mrbayes_output(node_set, tree_file):
    """Same as hdag output, but for MrBayes"""
    node2support = {}
    for tree in get_trprobs_trees(tree_file):
        rerooted = reroot(tree.search_nodes(name="ancestral")[0])
        node2cu = {}
        curr_internal_name = 0
        for node in rerooted.traverse("postorder"):
            if node.is_leaf():
                cu = [node.name]
            else:
                node.name = f"internal{curr_internal_name}"
                curr_internal_name += 1
                cu = []
                for child in node.children:
                    cu.extend(list(node2cu[child.name]))
            
            cu = frozenset(cu)
            node2cu[node.name] = cu
            if cu not in node2support:
                node2support[cu] = 0
            # The unrooted topologies in trprobs are unique, so we don't need
            # to worry about uniqueness when rerooting them:
            node2support[cu] += tree.prob

    return make_stats_list(node2support, node_set)

        
def beast_output(node_set, tree_file, num_trees=1e9):
    """Same as hdag output, but for BEAST.
    
    Command to test this on A.2.2/1:
python support_pipeline_scripts/cli.py save_supports \
-m "beast" \
-t "/home/whowards/hdag-benchmark/data/A.2.2/1/simulation/collapsed_simulated_tree.nwk" \
-i "/home/whowards/hdag-benchmark/data/A.2.2/1/results/beast/beast-output.trees" \
-o "/home/whowards/hdag-benchmark/data/A.2.2/1/results/beast/results.pkl"
    
    """

    # Precompute number of trees in BEAST run
    print("Counting number of lines...")
    num_lines = sum(1 for line in open(tree_file, "r")) # TODO: Figure out how to compute this more efficiently
    print("\tDone!")
    
    num_trees = num_lines
    burn_in = int(0.1 * num_trees)
    print(f"BEAST file has ~{num_trees} trees")


    total_trees = 0
    node2count = {}
    for i, tree in enumerate(iter_nexus_trees(tree_file)):
        if i % int(num_trees / 1000) == 0:
            print(f"\t{i} / {num_trees}")
        
        if i < burn_in:
            continue

        if i == burn_in:
            print(f"Finished burn-in. Considering approx {num_trees - burn_in} more trees.")

        # TODO: For smaller output debugging
        # if i > burn_in:
        #     break
        
        rerooted = reroot(tree.search_nodes(name="ancestral")[0])

        node2cu = {}
        curr_internal_name = 0
        for node in rerooted.traverse("postorder"):
            if node.is_leaf():
                cu = [node.name]
            else:
                node.name = f"internal{curr_internal_name}"
                curr_internal_name += 1
                cu = []
                for child in node.children:
                    cu.extend(list(node2cu[child.name]))
            
            cu = frozenset(cu)
            node2cu[node.name] = cu
            if cu not in node2count:
                node2count[cu] = 0
            node2count[cu] += 1
        total_trees += 1

    node2support = {}
    for node, count in node2count.items():
        if count > total_trees:
            print(f"=> Count is {count} with {total_trees} trees")
            print("Node")
            print(node)
        node2support[node] = count / total_trees

    return make_stats_list(node2support, node_set)


@click.command("trim_thresholds")
@click.option('--tree_path', '-t', help='the newick file for the true tree.')
@click.option('--pb_file', '-i', help='the file containing inference results.')
@click.option('--output_path', '-o', help='the file to save output to.')
def trim_thresholds(tree_path, pb_file, output_path):
    """
    Given a path to a completed hDAG (e.g., data/A.2.2/1/results/historydag/final_opt_dag.pb)
    store results.pkls for various trimming strategies
    """

    fasta_path = tree_path + ".fasta"
    taxId2seq = hdag.parsimony.load_fasta(fasta_path)
    node_set = get_true_nodes(tree_path)

    dag = hdag.mutation_annotated_dag.load_MAD_protobuf_file(pb_file)
    dag.make_complete()
    min_weight = dag.optimal_weight_annotate()

    strat_dict = {}

    strats = ["mp", 1.01, 1.05, 1.1, 1.2, "full"] # Proportion of parsimony to trim to
    for strat in strats:
        print(strat)
        if strats != "mp":
            dag = hdag.mutation_annotated_dag.load_MAD_protobuf_file(pb_file)
            dag.make_complete()
        
        if strat == "mp":
            dag.trim_optimal_weight()
        elif strat == "full":
            ... # Do no trimming
        else:
            max_weight = int(strat * min_weight)
            dag.trim_below_weight(max_weight=max_weight)
        
        trimmed_weights = dag.weight_count()
        # dag.summary()
        print("\tOmmitting dag.summary() for speed (?) ...")
        print()
        print()

        counts = dag.count_nodes(collapse=True)
        total_trees = dag.count_trees()

        seq2taxId = {v: k for k, v in taxId2seq.items()}

        node2stats = {}
        counter = 0
        for node in counts:
            if len(node) == 0:  # UA node
                continue
            # Convert cg label to taxon id
            id_node = frozenset([seq2taxId[label.compact_genome.to_sequence()] for label in node])
            est_sup = counts[node] / total_trees
            node2stats[id_node] = (est_sup, id_node in node_set)
        # Get the support for all nodes in true tree
        for id_node in node_set:
            if id_node not in node2stats.keys():
                node2stats[id_node] = (0, True)
        stats_list =[(id_node, stats[0], stats[1]) for id_node, stats in node2stats.items()]
        random.shuffle(stats_list)
        stats_list.sort(key=lambda el: el[1])

        strat_dict[strat] = (trimmed_weights, stats_list)

    with open(output_path, "wb") as f:
        pickle.dump(strat_dict, f)


@click.command("test_pars_weights")
@click.option('--tree_path', '-t', help='the newick file for the true tree.')
@click.option('--pb_file', '-i', help='the file containing inference results.')
@click.option('--output_path', '-o', help='the file to save output to.')
def test_pars_weights(tree_path, pb_file, output_path):
    """
    Given a path to a completed hDAG (e.g., data/A.2.2/1/results/historydag/final_opt_dag.pb)
    store results.pkls for various parsiomny weightings
    """

    fasta_path = tree_path + ".fasta"
    taxId2seq = hdag.parsimony.load_fasta(fasta_path)
    node_set = get_true_nodes(tree_path)

    weight_dict = {}
    pweight = [0, 1, 2, 3, 4, 10, 1e2, 1e3, 1e5, 1e10, "inf"] # Proportion of parsimony to trim to
    for p in pweight:
        stats_list = hdag_output_general(node_set, pb_file, taxId2seq, pars_weight=p)
        weight_dict[p] = (p, stats_list)

    with open(output_path, "wb") as f:
        pickle.dump(weight_dict, f)


@click.command("agg_pars_weights")
@click.option('--input', '-i', help='file path to input list as a pickle.')
@click.option('--out_dir', '-o', help='output directory to store figures/tables in.')
@click.option('--method', '-m', default='historydag')
@click.option('--clade_name', '-c')
@click.option('--window_proportion', '-w', default=0.20, help='the proportion of the data to use as window size')
def agg_pars_weights(input, out_dir, clade_name, method, window_proportion=0.20):
    """Given the pickled file, aggregates different inferences for various parsimony weights
    for computing support values
    
    E.g.
        python support_pipeline_scripts/cli.py agg_strats \
        -i /home/whowards/hdag-benchmark/data/A.2.2/1/results/historydag/strat_dict.pkl \
        -c A.2.2 \
        -o /home/whowards/hdag-benchmark/data/A.2.2/1/figures/historydag
    """

    try:
        with open(input, "rb") as f:
            strat_dict = pickle.load(f)
    except:
        print(f"\t...Skipping {input}")
        return

    # p_cmap = {
    #     0:#8dd3c7,
    #     1:#ffffb3,
    #     2:#bebada,
    #     3:#fb8072,
    #     4:#80b1d3,
    #     10:#fdb462,
    #     100:#b3de69,
    #     "inf":#fccde5
    # }

    # NOTE: Make sure the lengths match
    pweight = [0, 1, 2, 3, 4, 10, 1e2, 1e10, "inf"]
    colors = ['#fff7ec','#fee8c8','#fdd49e','#fdbb84','#fc8d59','#ef6548','#d7301f', '#000000', '#33a02c']
    p_cmap = {p: c for p, c in zip(pweight, colors)}

    
    for strat, (p, results) in strat_dict.items():
        if p not in pweight:
            continue
        # Remove leaf nodes
        res_no_leaves = []
        for el in results:
            if len(el[0]) > 1:
                res_no_leaves.append(el)
        results = res_no_leaves


        window_size = int(len(results) * window_proportion)

        # TODO: Change this to be for complete and complete_collapsed...
        out_path = out_dir + f"/pars_weight_w={window_size}_complete.png"
        x, y, min_sup, max_sup = sliding_window_plot(results, window_size=window_size, sup_range=True)

        # f, ax1 = plt.subplots(1, 1, sharex=True)

        plt.plot(x, y, color=p_cmap[p], label=p)

        # TODO: Add these back in once you have a better idea of the correct pweight
        # plt.scatter(min_sup, y, color=p_cmap[p], alpha=0.1)
        # plt.scatter(max_sup, y, color=p_cmap[p], alpha=0.1)
        # plt.fill_betweenx(y, min_sup, max_sup, alpha=0.1, color=p_cmap[p])

    plt.title(f"Clade {clade_name} Coverage Analysis with Varying Parsimony Weights")
    plt.ylabel(f"Empirical Probability (window_size={window_size}/{len(results)})")
    plt.xlabel("Estimated Support")
    plt.plot([0, 1], [0, 1], color="blue", label="Perfect")
    # plt.legend()
    plt.savefig(out_path)
    plt.clf()


@click.command("bin_pars_weights")
@click.option('--input', '-i', help='file path to input list as a pickle.')
@click.option('--out_dir', '-o', help='output directory to store figures/tables in.')
@click.option('--method', '-m', default='historydag')
@click.option('--clade_name', '-c')
@click.option('--bin_size', '-b', default=0.05, help='the proportion of the data to use as window size')
def bin_pars_weights(input, out_dir, clade_name, method, bin_size):
    """Given the pickled file, aggregates different inferences for various parsimony weights
    for computing support values USING A HISTOGRAM BINNING STRATEGY (AS OPPOSED TO SLIDING WINDOW)
    
    E.g.:
python support_pipeline_scripts/cli.py bin_pars_weights \
-i /home/whowards/hdag-benchmark/data/A.2.2/1/results/historydag/strat_dict_pars_weight.pkl \
-c A.2.2 \
-o /home/whowards/hdag-benchmark/data/A.2.2/1/figures/historydag
    """

    try:
        with open(input, "rb") as f:
            strat_dict = pickle.load(f)
    except:
        print(f"\t...Skipping {input}")
        return

    pweight = [0, 1, 2, 3, 4, 10, 1e2, 1e10, "inf"]
    colors = ['#fff7ec','#fee8c8','#fdd49e','#fdbb84','#fc8d59','#ef6548','#d7301f', '#000000', '#33a02c']
    p_cmap = {p: c for p, c in zip(pweight, colors)}

    
    for _, (p, results) in strat_dict.items():
        if p not in pweight:
            continue
        # Remove leaf nodes
        res_no_leaves = []
        for el in results:
            if len(el[0]) > 1:
                res_no_leaves.append(el)
        results = res_no_leaves

        # TODO: Change this to be for complete and complete_collapsed...
        out_path = out_dir + f"/pars_weight_binned_{bin_size}_collapse.png"
        x, y, std_pos, std_neg = bin_hist_plot(results, bin_size=bin_size)
        plt.plot(x, y, color=p_cmap[p], label=p)

        # TODO: Add these back in once you have a better idea of the correct pweight
        # plt.scatter(min_sup, y, color=p_cmap[p], alpha=0.1)
        # plt.scatter(max_sup, y, color=p_cmap[p], alpha=0.1)
        # plt.fill_betweenx(y, min_sup, max_sup, alpha=0.1, color=p_cmap[p])

    plt.title(f"Clade {clade_name} Coverage Analysis with Varying Parsimony Weights")
    plt.ylabel(f"Empirical Probability")
    plt.xlabel("Estimated Support")
    plt.plot([0, 1], [0, 1], color="blue", label="Perfect")
    # plt.legend()
    plt.savefig(out_path)
    plt.clf()


@click.command("agg_strats")
@click.option('--input', '-i', help='file path to input list as a pickle.')
@click.option('--out_dir', '-o', help='output directory to store figures/tables in.')
@click.option('--method', '-m', default='historydag')
@click.option('--clade_name', '-c')
@click.option('--window_proportion', '-w', default=0.20, help='the proportion of the data to use as window size')
def agg_strats(input, out_dir, clade_name, method, window_proportion=0.20):
    """Given the pickled file, aggregates different strategies for computing support values
    
    E.g.
        python support_pipeline_scripts/cli.py agg_strats \
        -i /home/whowards/hdag-benchmark/data/A.2.2/1/results/historydag/strat_dict.pkl \
        -c A.2.2 \
        -o /home/whowards/hdag-benchmark/data/A.2.2/1/figures/historydag

    """

    try:
        with open(input, "rb") as f:
            strat_dict = pickle.load(f)
    except:
        print(f"\t...Skipping {input}")
        return


    # TODO: Maybe we can use this type of plot to compare BEAST with MP-based support??

    mp_sups = []
    full_sups = []
    in_tree_labels = []

    mp_results = strat_dict["mp"][1]
    mp_dict = {node: (est_sup, in_tree) for node, est_sup, in_tree in mp_results}

    full_results = strat_dict["full"][1]
    for node, est_sup, in_tree in full_results:
        if node not in mp_dict:
            continue

        if len(node) == 1:
            print("Skipping leaf!")
            continue
        
        mp_sups.append(mp_dict[node][0])
        full_sups.append(est_sup)
        in_tree_labels.append(in_tree)

    x = [sup for sup, in_tree in zip(mp_sups, in_tree_labels) if in_tree]
    y = [sup for sup, in_tree in zip(full_sups, in_tree_labels) if in_tree]
    plt.scatter(x, y, label="In Tree", alpha=0.3)
    x = [sup for sup, in_tree in zip(mp_sups, in_tree_labels) if not in_tree]
    y = [sup for sup, in_tree in zip(full_sups, in_tree_labels) if not in_tree]
    plt.scatter(x, y, label="Not In Tree", alpha=0.3)
    plt.xlabel("MP-Trimmed Support")
    plt.ylabel("Full DAG Support")
    plt.title(f"Support Scatter")
    plt.plot([0, 1], [0, 1], color="green", linestyle="-", label="y = x", alpha=0.3)
    plt.legend()
    plt.savefig(out_dir + f"/mp_vs_full_support_comparison.png")
        

    
    for strat, (trimmed_weights, results) in strat_dict.items():
        # Remove leaf nodes
        res_no_leaves = []
        for el in results:
            if len(el[0]) > 1:
                res_no_leaves.append(el)
        results = res_no_leaves


        window_size = int(len(results) * window_proportion)

        out_path = out_dir + f"/support_quartiles_w={window_size}_strat={strat}.png"
        x, y, min_sup, max_sup = sliding_window_plot(results, window_size=window_size, sup_range=True)

        # print("est\ttrue\tQ1\tQ3")
        # for num, (i, j, k, l) in enumerate(zip(x, y, min_sup, max_sup)):
        #     print(f"{num}\t{i:3f}\t{j:3f}\t{k:3f}\t{l:3f}")
        # print(window_size)

        f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, height_ratios=[0.75, 0.25])
        f.set_size_inches(7, 10)

        ax1.set_title(f"Clade {clade_name} Coverage Analysis with \"{strat}\" Strategy")
        ax1.set_ylabel(f"Empirical Probability (window_size={window_size}/{len(results)})")

        ax1.plot(x, y, label="Support", color="red")
        ax1.scatter(min_sup, y, color="orange", alpha=0.1)
        ax1.scatter(max_sup, y, color="orange", alpha=0.1)
        # ax1.fill_betweenx(y, min_sup, max_sup, alpha=0.2, color="orange", label="Support Range")
        ax1.plot([0, 1], [0, 1], color="blue", label="Perfect")
        ax1.legend()
        
        ax2.hist(x)
        ax2.set_xlabel("Estimated Support")
        ax2.set_yscale("log")
        f.savefig(out_path)
        f.clf()

        f.set_size_inches(6, 8)
        plt.bar(list(trimmed_weights.keys()), list(trimmed_weights.values()))
        plt.xlabel("Parsimony")
        plt.yscale("log")
        plt.title(f"Parsiomny Distribution in DAG for \"{strat}\" Strategy")
        plt.savefig(out_dir + f"/parsiomny_distribution_for_{strat}.png")



# TODO: Implement a way to detect if you are slowing down, and then add the sample from any tree option
@click.command("larch_usher")
@click.option('--executable', '-e', default='/home/whowards/larch/larch/build/larch-usher', help='path to pre-made larch-usher executable')
@click.option('--input', '-i', help='input tree or hdag. if tree, need refseqfile.')
@click.option('--refseqfile', '-r', default=None, help='number of .')
@click.option('--count', '-c', help='number of iterations.')
@click.option('--out_dir', '-o', help='the directory for where to store resulting dag protobufs.')
@click.option('--schedule', '-s', default="annealed")
@click.option('--log_dir', '-l')
@click.option('--pars_score', '-p', default=1)
@click.option('--node_score', '-n', default=1)
def larch_usher(executable, input, refseqfile, count, out_dir, schedule, log_dir, pars_score, node_score):
    """Python CLI for driving larch-usher"""

    # E.g., results/historydag/
    os.chdir(f"{out_dir}")
    # subprocess.run(["cd", out_dir]) # One process can't change anothers working dir

    if int(count) <= 2:
        return

    print("\n\tCurrent directory in python:", out_dir)


    # Cast a wide net by prioritizing new nodes only
    print("Running initial iterations of larch-usher...")
    subprocess.run(["mkdir", "-p", f"{log_dir}_1"])
    args = [executable,
            "-i", input,
            "-c", f"{round(int(count)/2)}",
            "-o", f"{out_dir}/opt_dag_1.pb",
            "-l", f"{log_dir}_1",
            "--move-coeff-nodes", str(2),
            "--move-coeff-pscore", str(1),
            "--sample-best-tree"            # NOTE: Might need to change this with different version of larch-usher
            ]
    if refseqfile is not None:
        args.extend(["-r", refseqfile])
    subprocess.run(args=args)

    # Start considering parsimonious moves
    subprocess.run(["mkdir", "-p", f"{log_dir}_2"])
    args = [executable,
            "-i", f"{out_dir}/opt_dag_1.pb",
            "-c", f"{round(int(count)/6)}",
            "-o", f"{out_dir}/opt_dag_2.pb",
            "-l", f"{log_dir}_2",
            "--move-coeff-nodes", str(1),
            "--move-coeff-pscore", str(1),
            "--sample-best-tree"
            ]
    subprocess.run(args=args)

    # Emphasize parsimony over new nodes
    subprocess.run(["mkdir", "-p", f"{log_dir}_3"])
    args = [executable,
            "-i", f"{out_dir}/opt_dag_2.pb",
            "-c", f"{round(int(count)/6)}",
            "-o", f"{out_dir}/opt_dag_3.pb",
            "-l", f"{log_dir}_3",
            "--move-coeff-nodes", str(1),
            "--move-coeff-pscore", str(3),
            # "--sample-any-tree"
            ]
    subprocess.run(args=args)

    print("Completing DAG...")
    dag = hdag.mutation_annotated_dag.load_MAD_protobuf_file(f"{out_dir}/opt_dag_3.pb")
    dag.make_complete()
    dag.to_protobuf_file(f"{out_dir}/complete_opt_dag.pb")

    subprocess.run(["mkdir", "-p", f"{log_dir}_complete"])
    args = [executable,
            "-i", f"{out_dir}/complete_opt_dag.pb",
            "-c", f"{round(int(count)/6)}",
            "-o", f"{out_dir}/final_opt_dag.pb",
            "-l", f"{log_dir}_complete",
            "--move-coeff-nodes", str(1),
            "--move-coeff-pscore", str(3),
            "--sample-best-tree"
            ]
    subprocess.run(args=args)


    # TODO: Add --sample-any again?




###################################################################################################
#### Aggregation ##################################################################################
###################################################################################################

# TODO: Rename this
@click.command("agg")
@click.option('--input', '-i', help='file path to input list as a pickle.')
@click.option('--out_dir', '-o', help='output directory to store figures/tables in.')
@click.option('--method', '-m', default='historydag')
@click.option('--clade_name', '-c')
@click.option('--window_proportion', '-w', default=0.20, help='the proportion of the data to use as window size')
def agg(input, out_dir, clade_name, method, window_proportion=0.20):
    """Given the pickled file, aggregates results for support values"""

    try:
        with open(input, "rb") as f:
            results = pickle.load(f)
    except:
        print(f"\t...Skipping {input}")
        return

    # Remove leaf nodes
    res_no_leaves = []
    for el in results:
        if len(el[0]) > 1:
            res_no_leaves.append(el)
    results = res_no_leaves


    if method == "beast":
        window_size = 100
    else:
        window_size = int(len(results) * window_proportion)

    out_path = out_dir + f"/support_quartiles_w={window_size}.png"
    x, y, min_sup, max_sup = sliding_window_plot(results, window_size=window_size, sup_range=True)

    # print("est\ttrue\tQ1\tQ3")
    # for num, (i, j, k, l) in enumerate(zip(x, y, min_sup, max_sup)):
    #     print(f"{num}\t{i:3f}\t{j:3f}\t{k:3f}\t{l:3f}")
    # print(window_size)

    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, height_ratios=[0.75, 0.25])
    f.set_size_inches(7, 9)

    ax1.set_title(f"Clade {clade_name} Coverage Analysis")
    ax1.set_ylabel(f"Empirical Probability (window_size={window_size}/{len(results)})")

    ax1.plot(x, y, label="Support", color="red")
    ax1.scatter(min_sup, y, color="orange", alpha=0.1)
    ax1.scatter(max_sup, y, color="orange", alpha=0.1)
    # ax1.fill_betweenx(y, min_sup, max_sup, alpha=0.2, color="orange", label="Support Range")
    ax1.plot([0, 1], [0, 1], color="blue", label="Perfect")
    ax1.legend()
    
    ax2.hist(x)
    ax2.set_xlabel("Estimated Support")
    ax2.set_yscale("log")
    f.savefig(out_path)
    f.clf()



@click.command("clade_results")
@click.option('--clade_dir', '-c', help='path to clade directory.')
@click.option('--out_dir', '-o', help='output directory to store figures/tables in.')
@click.option('--num_sim', '-n', default=1, help='number of simulations to average.')
@click.option('--method', '-m', default='historydag')
@click.option('--window_proportion', '-w', default=0.20, help='the proportion of the data to use as window size')
def clade_results(clade_dir, out_dir, num_sim, method, window_proportion):
    """Given the clade directory, performs coverage analysis across all simulations"""

    clade_name = clade_dir.split("/")[-1]

    # Multi-line Plot
    result_dict = {}
    for trial in range(1, num_sim+1):
        # Assumes that `path/to/clade/trial/results/results.pkl stores`` list of nodes
        #   their supports and whether they're in the true tree or not
        result_path = clade_dir + f"/{trial}/results/{method}/results.pkl"
        
        try:
            with open(result_path, "rb") as f:
                results = pickle.load(f)
                result_dict[trial] = results
        except:
            print(f"\tSkipping {clade_dir} {trial}")
            continue
    
    if len(result_dict) == 0:
        print("\n==>No results to print :(\n")
        return

    avg_window_size = 0
    avg_results_length = 0
    for trial in result_dict.keys():
        result = result_dict[trial]

        if method == "beast":
            window_size = 100
        else:
            window_size = int(len(result) * window_proportion)
        
        avg_window_size += window_size
        avg_results_length += len(result)
        x, y = sliding_window_plot(result, window_size=window_size)
        plt.plot(x, y)
    
    avg_results_length /= int(len(result_dict))
    avg_window_size /= int(len(result_dict))

    plt.plot([0, 1], [0, 1])
    plt.xlabel("Estimated Support")
    plt.ylabel(f"Empirical Probability (window_size~{int(avg_window_size)}/{int(avg_results_length)})")
    plt.title(f"Aggregated {clade_name} Coverage Analysis")
    out_path = out_dir + f"/multi_line_w={int(avg_window_size)}.png"
    plt.savefig(out_path)
    plt.clf()



    # Single-line Plot

    results_full = []
    for trial, results in result_dict.items():
        toi_node_count = 0
        num_leaves = 0
        for result in results:
            if result[1] > 1.0:
                print("node length:", len(node))
                print("trial:", trial)
                continue

            node = result[0]
            if len(node) > 1:       # Removing leaves
                results_full.append(result)
                if result[2]:
                    toi_node_count += 1
            else:
                # Checking that all leaves are in true tree
                assert result[2]
                num_leaves += 1

        
        # NOTE: Uncomment if you want clade size info about each tree
        # print(f"{clade_name}/{trial} {toi_node_count} non-leaf nodes with {num_leaves} leaves")
    
    print(f"\tsorting {len(results_full)} results...")
    random.shuffle(results_full)
    results_full.sort(key=lambda el: el[1])


    if method == "beast":
        window_size = 100
    else:
        window_size = int(len(results_full) * window_proportion)

    out_path = out_dir + f"/single_line_w={window_size}.png"
    print(f"\tgenerating window plot at {out_path}...")
    x, y, pos_devs, neg_devs = sliding_window_plot(results_full, std_dev=True, window_size=window_size)

    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, height_ratios=[0.75, 0.25])
    f.set_size_inches(7, 9)
    ax1.set_title(f"Clade {clade_name} Coverage Analysis")

    ax1.set_ylabel(f"Empirical Probability (window_size={window_size}/{len(results_full)})")
    ax1.plot(x, y, color="orange", label="Support Regressor")
    ax1.fill_between(x, pos_devs, neg_devs, alpha=0.2, color="orange")
    ax1.plot([0, 1], [0, 1], color="blue", label="Perfect Regressor")
    ax1.legend()
    
    ax2.hist(x)
    ax2.set_yscale("log")
    ax2.set_xlabel("Estimated Support")
    f.savefig(out_path)
    f.clf()


def bin_hist_plot(results, bin_size=0.05):
    """Given list of results tuples returns xy coords of sliding window plot."""

    results.sort(key= lambda result: result[1])

    x, y, devs = [], [], []
    
    
    ind_max = 0
    for bin in range(0, 100, int(bin_size * 100)): # NOTE: this is in percent
        est_min = bin / 100
        est_max = bin / 100 + bin_size

        ind_min = ind_max   # Use previous maximum index
        for i, el in enumerate(results[ind_min:]):
            if el[1] >= est_max and est_max != 1:
                break
        ind_max = ind_min + i
        if i == 0:
            # There are no elements in this bin
            # print(f"\t -- Skipping bin [{est_min}, {est_max}]")
            continue

        # print(f"\t bin [{est_min}, {est_max}]")

        in_tree_window = [int(el[2]) for el in results[ind_min:ind_max]]
        avg = sum(in_tree_window) / len(in_tree_window)
        y.append(avg)
        est_sup_window = [el[1] for el in results[ind_min:ind_max]]
        x.append(sum(est_sup_window) / len(est_sup_window))

        # Standard deviations are over the window of in_true_tree variable (ie 1 or 0)
        sq_diff = [(el - avg)**2 for el in in_tree_window]
        devs.append((sum(sq_diff) / len(sq_diff)))

    pos_devs = [y_val + dev for y_val, dev in zip(y, devs)]
    neg_devs = [y_val - dev for y_val, dev in zip(y, devs)]

    # print(f"\tx: {x}\ty: {y}")
    return x, y, pos_devs, neg_devs

@click.command("binned_clade_results")
@click.option('--clade_dir', '-c', help='path to clade directory.')
@click.option('--out_dir', '-o', help='output directory to store figures/tables in.')
@click.option('--num_sim', '-n', default=1, help='number of simulations to average.')
@click.option('--method', '-m', default='historydag')
@click.option('--bin_size', '-b', default=0.05, help='the proportion of the data to use as window size')
def binned_clade_results(clade_dir, out_dir, num_sim, method, bin_size):
    """Given the clade directory, performs coverage analysis across all simulations"""

    clade_name = clade_dir.split("/")[-1]

    result_dict = {}
    for trial in range(1, num_sim+1):
        # Assumes that `path/to/clade/trial/results/results.pkl stores`` list of nodes
        #   their supports and whether they're in the true tree or not
        result_path = clade_dir + f"/{trial}/results/{method}/strat_dict_node_weight.pkl"
        
        try:
            with open(result_path, "rb") as f:
                results = pickle.load(f)
                result_dict[trial] = results
        except:
            print(f"\tSkipping {clade_dir} {trial}")
            continue
    
    if len(result_dict) == 0:
        print("\n==> No results to print :(\n")
        return

    results_full = {p: [] for p in result_dict[1].keys()}
    for trial, strat_dict in result_dict.items():
        for p, results in strat_dict.items():
            results_full[p].extend(results[1])

    
    # print(f"\tsorting {len(results_full)} results...")
    for _, results in results_full.items():
        random.shuffle(results)
        results.sort(key=lambda el: el[1])

    bin_size = 0.05
    plt.plot([0, 1], [0, 1], color="blue", label="Perfect")
    pweight = [0, 1, 2, 3, 4, 10, 1e2, 1e10, "inf"]
    colors = ['#fff7ec','#fee8c8','#fdd49e','#fdbb84','#fc8d59','#ef6548','#d7301f', '#000000', '#33a02c']
    p_cmap = {p: c for p, c in zip(pweight, colors)}

    for p, results in results_full.items():
        if p not in pweight:
            continue
        # Remove leaf nodes
        res_no_leaves = []
        for el in results:
            if len(el[0]) > 1:
                res_no_leaves.append(el)
        results = res_no_leaves

        out_path = out_dir + f"/single_line_node_weighted_w={200}.png" # f"/pars_weight_binned_{bin_size}.png"
        x, y, std_pos, std_neg = sliding_window_plot(results, std_dev=True, window_size=200) # bin_hist_plot(results, bin_size=bin_size)
        plt.plot(x, y, color=p_cmap[p], label=p)

        # TODO: Add these back in once you have a better idea of the correct pweight
        # plt.scatter(min_sup, y, color=p_cmap[p], alpha=0.1)
        # plt.scatter(max_sup, y, color=p_cmap[p], alpha=0.1)
        # plt.fill_betweenx(y, min_sup, max_sup, alpha=0.1, color=p_cmap[p])

    plt.title(f"Clade {clade_name} Coverage Analysis with Varying Parsimony Weights")
    plt.ylabel(f"Empirical Probability")
    plt.xlabel("Estimated Support")
    # plt.legend()
    plt.savefig(out_path)
    plt.clf()


def sliding_window_plot(results, std_dev=False, sup_range=False, window_size=200):
    """Given list of results tuples returns xy coords of sliding window plot."""

    results.sort(key= lambda result: result[1])

    x, y = [], []
    devs, min_sup, max_sup = [], [], []
    side_len = int(window_size/2)
    for i, (_, est_sup, in_tree) in enumerate(results):
        x.append(est_sup)
        window = [int(el[2]) for el in results[max(0, i-side_len):min(len(results), i+side_len)]]
        y.append(sum(window) / len(window))
        
        # Standard deviations are over the window of in_true_tree variable (ie 1 or 0)
        if std_dev:
            avg = y[i]
            sq_diff = [(el - avg)**2 for el in window]
            devs.append((sum(sq_diff) / len(sq_diff)))

        # Quartiles are between estimated support values
        elif sup_range:
            support_window = [el[1] for el in results[max(0, i-side_len):min(len(results), i+side_len)]]
            # First and fourth quartiles
            min_sup.append(support_window[int(len(support_window)/4)])
            max_sup.append(support_window[-int(len(support_window)/4)])

    if std_dev:
        pos_devs = [y_val + dev for y_val, dev in zip(y, devs)]
        neg_devs = [y_val - dev for y_val, dev in zip(y, devs)]
        return x, y, pos_devs, neg_devs
    elif sup_range:
        return x, y, min_sup, max_sup
    else:
        return x, y

cli.add_command(test_pars_weights)
cli.add_command(trim_thresholds)
cli.add_command(parse_clade_stats)
cli.add_command(get_pars_score)
cli.add_command(clade_results)
cli.add_command(save_supports)
cli.add_command(larch_usher)
cli.add_command(agg)
cli.add_command(agg_strats)
cli.add_command(agg_pars_weights)
cli.add_command(bin_pars_weights)
cli.add_command(binned_clade_results)

if __name__ == '__main__':
    cli()
